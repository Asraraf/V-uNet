{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nn_compat.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6bIIKDSodwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "modified from pixelcnn++\n",
        "Various tensorflow utilities\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib.framework.python.ops import add_arg_scope\n",
        "\n",
        "\n",
        "def int_shape(x):\n",
        "    return x.shape.as_list()\n",
        "\n",
        "\n",
        "def get_name(layer_name, counters):\n",
        "    ''' utlity for keeping track of layer names '''\n",
        "    if not layer_name in counters:\n",
        "        counters[layer_name] = 0\n",
        "    name = layer_name + '_' + str(counters[layer_name])\n",
        "    counters[layer_name] += 1\n",
        "    return name\n",
        "\n",
        "\n",
        "@add_arg_scope\n",
        "def dense(x, num_units, init_scale=1., counters={}, init=False, **kwargs):\n",
        "    ''' fully connected layer '''\n",
        "    name = get_name('dense', counters)\n",
        "    with tf.variable_scope(name):\n",
        "        if init:\n",
        "            xs = x.shape.as_list()\n",
        "            # data based initialization of parameters\n",
        "            V = tf.get_variable('V', [xs[1], num_units], tf.float32, tf.random_normal_initializer(0, 0.05))\n",
        "            V_norm = tf.nn.l2_normalize(V.initialized_value(), [0])\n",
        "            x_init = tf.matmul(x, V_norm)\n",
        "            m_init, v_init = tf.nn.moments(x_init, [0])\n",
        "            scale_init = init_scale / tf.sqrt(v_init + 1e-10)\n",
        "            g = tf.get_variable('g', dtype=tf.float32, initializer=scale_init)\n",
        "            b = tf.get_variable('b', dtype=tf.float32, initializer=-m_init * scale_init)\n",
        "            x_init = tf.reshape(scale_init, [1, num_units]) * (x_init - tf.reshape(m_init, [1, num_units]))\n",
        "\n",
        "            return x_init\n",
        "        else:\n",
        "            V = tf.get_variable(\"V\")\n",
        "            g = tf.get_variable(\"g\")\n",
        "            b = tf.get_variable(\"b\")\n",
        "            with tf.control_dependencies([tf.assert_variables_initialized([V, g, b])]):\n",
        "                # use weight normalization (Salimans & Kingma, 2016)\n",
        "                x = tf.matmul(x, V)\n",
        "                scaler = g / tf.sqrt(tf.reduce_sum(tf.square(V), [0]))\n",
        "                x = tf.reshape(scaler, [1, num_units]) * x + tf.reshape(b, [1, num_units])\n",
        "\n",
        "                return x\n",
        "\n",
        "\n",
        "@add_arg_scope\n",
        "def conv2d(x, num_filters, filter_size=[3, 3], stride=[1, 1], pad='SAME', init_scale=1., counters={}, init=False, **kwargs):\n",
        "    ''' convolutional layer '''\n",
        "    num_filters = int(num_filters)\n",
        "    strides = [1] + stride + [1]\n",
        "    name = get_name('conv2d', counters)\n",
        "    with tf.variable_scope(name):\n",
        "        if init:\n",
        "            xs = x.shape.as_list()\n",
        "            # data based initialization of parameters\n",
        "            V = tf.get_variable('V', filter_size + [xs[-1], num_filters],\n",
        "                                tf.float32, tf.random_normal_initializer(0, 0.05))\n",
        "            V_norm = tf.nn.l2_normalize(V.initialized_value(), [0, 1, 2])\n",
        "            x_init = tf.nn.conv2d(x, V_norm, strides, pad)\n",
        "            m_init, v_init = tf.nn.moments(x_init, [0, 1, 2])\n",
        "            scale_init = init_scale / tf.sqrt(v_init + 1e-8)\n",
        "            g = tf.get_variable('g', dtype=tf.float32, initializer = scale_init)\n",
        "            b = tf.get_variable('b', dtype=tf.float32, initializer = -m_init * scale_init)\n",
        "            x_init = tf.reshape(scale_init, [1, 1, 1, num_filters]) * (x_init - tf.reshape(m_init, [1, 1, 1, num_filters]))\n",
        "\n",
        "            return x_init\n",
        "        else:\n",
        "            V = tf.get_variable(\"V\")\n",
        "            g = tf.get_variable(\"g\")\n",
        "            b = tf.get_variable(\"b\")\n",
        "            with tf.control_dependencies([tf.assert_variables_initialized([V, g, b])]):\n",
        "                # use weight normalization (Salimans & Kingma, 2016)\n",
        "                W = tf.reshape(g, [1, 1, 1, num_filters]) * tf.nn.l2_normalize(V, [0, 1, 2])\n",
        "\n",
        "                # calculate convolutional layer output\n",
        "                x = tf.nn.bias_add(tf.nn.conv2d(x, W, strides, pad), b)\n",
        "\n",
        "                return x\n",
        "\n",
        "\n",
        "@add_arg_scope\n",
        "def deconv2d(x, num_filters, filter_size=[3, 3], stride=[1, 1], pad='SAME', init_scale=1., counters={}, init=False, **kwargs):\n",
        "    ''' transposed convolutional layer '''\n",
        "    num_filters = int(num_filters)\n",
        "    name = get_name('deconv2d', counters)\n",
        "    xs = int_shape(x)\n",
        "    strides = [1] + stride + [1]\n",
        "    if pad == 'SAME':\n",
        "        target_shape = [xs[0], xs[1] * stride[0],\n",
        "                        xs[2] * stride[1], num_filters]\n",
        "    else:\n",
        "        target_shape = [xs[0], xs[1] * stride[0] + filter_size[0] -\n",
        "                        1, xs[2] * stride[1] + filter_size[1] - 1, num_filters]\n",
        "    with tf.variable_scope(name):\n",
        "        if init:\n",
        "            # data based initialization of parameters\n",
        "            V = tf.get_variable('V', filter_size + [num_filters, xs[-1]], tf.float32, tf.random_normal_initializer(0, 0.05))\n",
        "            V_norm = tf.nn.l2_normalize(V.initialized_value(), [0, 1, 3])\n",
        "            x_init = tf.nn.conv2d_transpose(x, V_norm, target_shape, strides, padding=pad)\n",
        "            m_init, v_init = tf.nn.moments(x_init, [0, 1, 2])\n",
        "            scale_init = init_scale / tf.sqrt(v_init + 1e-8)\n",
        "            g = tf.get_variable('g', dtype=tf.float32, initializer=scale_init)\n",
        "            b = tf.get_variable('b', dtype=tf.float32, initializer=-m_init * scale_init)\n",
        "            x_init = tf.reshape(scale_init, [1, 1, 1, num_filters]) * (x_init - tf.reshape(m_init, [1, 1, 1, num_filters]))\n",
        "\n",
        "            return x_init\n",
        "        else:\n",
        "            V = tf.get_variable(\"V\")\n",
        "            g = tf.get_variable(\"g\")\n",
        "            b = tf.get_variable(\"b\")\n",
        "            with tf.control_dependencies([tf.assert_variables_initialized([V, g, b])]):\n",
        "                # use weight normalization (Salimans & Kingma, 2016)\n",
        "                W = tf.reshape(g, [1, 1, num_filters, 1]) * tf.nn.l2_normalize(V, [0, 1, 3])\n",
        "\n",
        "                # calculate convolutional layer output\n",
        "                x = tf.nn.conv2d_transpose(x, W, target_shape, strides, padding=pad)\n",
        "                x = tf.nn.bias_add(x, b)\n",
        "\n",
        "                return x\n",
        "\n",
        "\n",
        "@add_arg_scope\n",
        "def activate(x, activation, **kwargs):\n",
        "    if activation == None:\n",
        "        return x\n",
        "    elif activation == \"elu\":\n",
        "        return tf.nn.elu(x)\n",
        "    else:\n",
        "        raise NotImplemented(activation)\n",
        "\n",
        "\n",
        "def nin(x, num_units):\n",
        "    \"\"\" a network in network layer (1x1 CONV) \"\"\"\n",
        "    s = int_shape(x)\n",
        "    x = tf.reshape(x, [np.prod(s[:-1]), s[-1]])\n",
        "    x = dense(x, num_units)\n",
        "    return tf.reshape(x, s[:-1] + [num_units])\n",
        "\n",
        "\n",
        "def downsample(x, num_units):\n",
        "    return conv2d(x, num_units, stride = [2, 2])\n",
        "\n",
        "\n",
        "def upsample(x, num_units, method = \"subpixel\"):\n",
        "    if method == \"conv_transposed\":\n",
        "        return deconv2d(x, num_units, stride = [2, 2])\n",
        "    elif method == \"subpixel\":\n",
        "        x = conv2d(x, 4*num_units)\n",
        "        x = tf.depth_to_space(x, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "@add_arg_scope\n",
        "def residual_block(x, a = None, conv=conv2d, init=False, dropout_p=0.0, gated = False, **kwargs):\n",
        "    \"\"\"Slight variation of original.\"\"\"\n",
        "    xs = int_shape(x)\n",
        "    num_filters = xs[-1]\n",
        "\n",
        "    residual = x\n",
        "    if a is not None:\n",
        "        a = nin(activate(a), num_filters)\n",
        "        residual = tf.concat([residual, a], axis = -1)\n",
        "    residual = activate(residual)\n",
        "    residual = tf.nn.dropout(residual, keep_prob = 1.0 - dropout_p)\n",
        "    residual = conv(residual, num_filters)\n",
        "    if gated:\n",
        "        residual = activate(residual)\n",
        "        residual = tf.nn.dropout(residual, keep_prob = 1.0 - dropout_p)\n",
        "        residual = conv(residual, 2*num_filters)\n",
        "        a, b = tf.split(residual, 2, 3)\n",
        "        residual = a * tf.nn.sigmoid(b)\n",
        "\n",
        "    return x + residual\n",
        "\n",
        "\n",
        "def make_linear_var(\n",
        "        step,\n",
        "        start, end,\n",
        "        start_value, end_value,\n",
        "        clip_min = 0.0, clip_max = 1.0):\n",
        "    \"\"\"linear from (a, alpha) to (b, beta), i.e.\n",
        "    (beta - alpha)/(b - a) * (x - a) + alpha\"\"\"\n",
        "    linear = (\n",
        "            (end_value - start_value) /\n",
        "            (end - start) *\n",
        "            (tf.cast(step, tf.float32) - start) + start_value)\n",
        "    return tf.clip_by_value(linear, clip_min, clip_max)\n",
        "\n",
        "\n",
        "def split_groups(x, bs = 2):\n",
        "    return tf.split(tf.space_to_depth(x, bs), bs**2, axis = 3)\n",
        "\n",
        "\n",
        "def merge_groups(xs, bs = 2):\n",
        "    return tf.depth_to_space(tf.concat(xs, axis = 3), bs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
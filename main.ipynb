{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Asraraf/V-uNet/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xkoJxsbn74f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = False\n",
        "session = tf.Session(config = config)\n",
        "\n",
        "import os, logging, shutil, datetime\n",
        "import glob\n",
        "import argparse\n",
        "import yaml\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import nn\n",
        "import models\n",
        "from batches import get_batches, plot_batch, postprocess, n_boxes\n",
        "import deeploss\n",
        "\n",
        "\n",
        "def init_logging(out_base_dir):\n",
        "    # get unique output directory based on current time\n",
        "    os.makedirs(out_base_dir, exist_ok = True)\n",
        "    now = datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
        "    out_dir = os.path.join(out_base_dir, now)\n",
        "    os.makedirs(out_dir, exist_ok = False)\n",
        "    # copy source code to logging dir to have an idea what the run was about\n",
        "    this_file = os.path.realpath(__file__)\n",
        "    assert(this_file.endswith(\".py\"))\n",
        "    shutil.copy(this_file, out_dir)\n",
        "    # copy all py files to logging dir\n",
        "    src_dir = os.path.dirname(this_file)\n",
        "    py_files = glob.glob(os.path.join(src_dir, \"*.py\"))\n",
        "    for py_file in py_files:\n",
        "        shutil.copy(py_file, out_dir)\n",
        "    # init logging\n",
        "    logging.basicConfig(filename = os.path.join(out_dir, 'log.txt'))\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "    return out_dir, logger\n",
        "\n",
        "\n",
        "class Model(object):\n",
        "    def __init__(self, config, out_dir, logger):\n",
        "        self.config = config\n",
        "        self.batch_size = config[\"batch_size\"]\n",
        "        self.img_shape = 2*[config[\"spatial_size\"]] + [3]\n",
        "        self.bottleneck_factor = config[\"bottleneck_factor\"]\n",
        "        self.box_factor = config[\"box_factor\"]\n",
        "        self.imgn_shape = 2*[config[\"spatial_size\"]//(2**self.box_factor)] + [n_boxes*3]\n",
        "        self.init_batches = config[\"init_batches\"]\n",
        "\n",
        "        self.initial_lr = config[\"lr\"]\n",
        "        self.lr_decay_begin = config[\"lr_decay_begin\"]\n",
        "        self.lr_decay_end = config[\"lr_decay_end\"]\n",
        "\n",
        "        self.out_dir = out_dir\n",
        "        self.logger = logger\n",
        "        self.log_frequency = config[\"log_freq\"]\n",
        "        self.ckpt_frequency = config[\"ckpt_freq\"]\n",
        "        self.test_frequency = config[\"test_freq\"]\n",
        "        self.checkpoint_best = False\n",
        "\n",
        "        self.dropout_p = config[\"drop_prob\"]\n",
        "\n",
        "        self.best_loss = float(\"inf\")\n",
        "        self.checkpoint_dir = os.path.join(self.out_dir, \"checkpoints\")\n",
        "        os.makedirs(self.checkpoint_dir, exist_ok = True)\n",
        "\n",
        "        self.define_models()\n",
        "        self.define_graph()\n",
        "\n",
        "\n",
        "    def define_models(self):\n",
        "        n_latent_scales = 2\n",
        "        n_scales = 1 + int(np.round(np.log2(self.img_shape[0]))) - self.bottleneck_factor\n",
        "        n_filters = 32\n",
        "        self.enc_up_pass = models.make_model(\n",
        "                \"enc_up\", models.enc_up,\n",
        "                n_scales = n_scales - self.box_factor,\n",
        "                n_filters = n_filters*2**self.box_factor)\n",
        "        self.enc_down_pass = models.make_model(\n",
        "                \"enc_down\", models.enc_down,\n",
        "                n_scales = n_scales - self.box_factor,\n",
        "                n_latent_scales = n_latent_scales)\n",
        "        self.dec_up_pass = models.make_model(\n",
        "                \"dec_up\", models.dec_up,\n",
        "                n_scales = n_scales,\n",
        "                n_filters = n_filters)\n",
        "        self.dec_down_pass = models.make_model(\n",
        "                \"dec_down\", models.dec_down,\n",
        "                n_scales = n_scales,\n",
        "                n_latent_scales = n_latent_scales)\n",
        "        self.dec_params = models.make_model(\n",
        "                \"dec_params\", models.dec_parameters)\n",
        "\n",
        "\n",
        "    def train_forward_pass(self, x, c, xn, cn, dropout_p, init = False):\n",
        "        kwargs = {\"init\": init, \"dropout_p\": dropout_p}\n",
        "        # encoder\n",
        "        hs = self.enc_up_pass(xn, cn, **kwargs)\n",
        "        es, qs, zs_posterior = self.enc_down_pass(hs, **kwargs)\n",
        "        # decoder\n",
        "        gs = self.dec_up_pass(c, **kwargs)\n",
        "        ds, ps, zs_prior = self.dec_down_pass(gs, zs_posterior, training = True, **kwargs)\n",
        "        params = self.dec_params(ds[-1], **kwargs)\n",
        "        activations = hs + es + gs + ds\n",
        "        return params, qs, ps, activations\n",
        "\n",
        "\n",
        "    def test_forward_pass(self, c):\n",
        "        kwargs = {\"init\": False, \"dropout_p\": 0.0}\n",
        "        # decoder\n",
        "        gs = self.dec_up_pass(c, **kwargs)\n",
        "        ds, ps, zs_prior = self.dec_down_pass(gs, [], training = False, **kwargs)\n",
        "        params = self.dec_params(ds[-1], **kwargs)\n",
        "        return params\n",
        "\n",
        "\n",
        "    def transfer_pass(self, infer_x, infer_c, generate_c):\n",
        "        kwargs = {\"init\": False, \"dropout_p\": 0.0}\n",
        "        # infer latent code\n",
        "        hs = self.enc_up_pass(infer_x, infer_c, **kwargs)\n",
        "        es, qs, zs_posterior = self.enc_down_pass(hs, **kwargs)\n",
        "        zs_mean = list(qs)\n",
        "        # generate from inferred latent code and conditioning\n",
        "        gs = self.dec_up_pass(generate_c, **kwargs)\n",
        "        use_mean = True\n",
        "        if use_mean:\n",
        "            ds, ps, zs_prior = self.dec_down_pass(gs, zs_mean, training = True, **kwargs)\n",
        "        else:\n",
        "            ds, ps, zs_prior = self.dec_down_pass(gs, zs_posterior, training = True, **kwargs)\n",
        "        params = self.dec_params(ds[-1], **kwargs)\n",
        "        return params\n",
        "\n",
        "\n",
        "    def sample(self, params, **kwargs):\n",
        "        return params\n",
        "\n",
        "\n",
        "    def likelihood_loss(self, x, params):\n",
        "        return 5.0*self.vgg19.make_loss_op(x, params)\n",
        "\n",
        "\n",
        "    def define_graph(self):\n",
        "        # pretrained net for perceptual loss\n",
        "        self.vgg19 = deeploss.VGG19Features(session,\n",
        "                feature_layers = self.config[\"feature_layers\"],\n",
        "                feature_weights = self.config[\"feature_weights\"],\n",
        "                gram_weights = self.config[\"gram_weights\"])\n",
        "\n",
        "        global_step = tf.Variable(0, trainable = False, name = \"global_step\")\n",
        "        lr = nn.make_linear_var(\n",
        "                global_step,\n",
        "                self.lr_decay_begin, self.lr_decay_end,\n",
        "                self.initial_lr, 0.0,\n",
        "                0.0, self.initial_lr)\n",
        "        kl_weight = nn.make_linear_var(\n",
        "                global_step,\n",
        "                self.lr_decay_end // 2, 3 * self.lr_decay_end // 4,\n",
        "                1e-6, 1.0,\n",
        "                1e-6, 1.0)\n",
        "\n",
        "        # initialization\n",
        "        self.x_init = tf.placeholder(\n",
        "                tf.float32,\n",
        "                shape = [self.init_batches * self.batch_size] + self.img_shape)\n",
        "        self.c_init = tf.placeholder(\n",
        "                tf.float32,\n",
        "                shape = [self.init_batches * self.batch_size] + self.img_shape)\n",
        "        self.xn_init = tf.placeholder(\n",
        "                tf.float32,\n",
        "                shape = [self.init_batches * self.batch_size] + self.imgn_shape)\n",
        "        self.cn_init = tf.placeholder(\n",
        "                tf.float32,\n",
        "                shape = [self.init_batches * self.batch_size] + self.imgn_shape)\n",
        "        self.dd_init_op = self.train_forward_pass(\n",
        "                self.x_init, self.c_init,\n",
        "                self.xn_init, self.cn_init,\n",
        "                dropout_p = self.dropout_p, init = True)\n",
        "\n",
        "        # training\n",
        "        self.x = tf.placeholder(\n",
        "                tf.float32,\n",
        "                shape = [self.batch_size] + self.img_shape)\n",
        "        self.c = tf.placeholder(\n",
        "                tf.float32,\n",
        "                shape = [self.batch_size] + self.img_shape)\n",
        "        self.xn = tf.placeholder(\n",
        "                tf.float32,\n",
        "                shape = [self.batch_size] + self.imgn_shape)\n",
        "        self.cn = tf.placeholder(\n",
        "                tf.float32,\n",
        "                shape = [self.batch_size] + self.imgn_shape)\n",
        "        # compute parameters of model distribution\n",
        "        params, qs, ps, activations = self.train_forward_pass(\n",
        "                self.x, self.c,\n",
        "                self.xn, self.cn,\n",
        "                dropout_p = self.dropout_p)\n",
        "        # sample from model distribution\n",
        "        sample = self.sample(params)\n",
        "        # maximize likelihood\n",
        "        likelihood_loss = self.likelihood_loss(self.x, params)\n",
        "        kl_loss = tf.to_float(0.0)\n",
        "        for q, p in zip(qs, ps):\n",
        "            self.logger.info(\"Latent shape: {}\".format(q.shape.as_list()))\n",
        "            kl_loss += models.latent_kl(q, p)\n",
        "        loss = likelihood_loss + kl_weight * kl_loss\n",
        "\n",
        "        # testing\n",
        "        test_forward = self.test_forward_pass(self.c)\n",
        "        test_sample = self.sample(test_forward)\n",
        "\n",
        "        # reconstruction\n",
        "        reconstruction_params, _, _, _ = self.train_forward_pass(\n",
        "                self.x, self.c,\n",
        "                self.xn, self.cn,\n",
        "                dropout_p = 0.0)\n",
        "        self.reconstruction = self.sample(reconstruction_params)\n",
        "\n",
        "        # optimization\n",
        "        self.trainable_variables = [v for v in tf.trainable_variables()\n",
        "                if not v in self.vgg19.variables]\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate = lr, beta1 = 0.5, beta2 = 0.9)\n",
        "        opt_op = optimizer.minimize(loss, var_list = self.trainable_variables)\n",
        "        with tf.control_dependencies([opt_op]):\n",
        "            self.train_op = tf.assign(global_step, global_step + 1)\n",
        "\n",
        "\n",
        "        # logging and visualization\n",
        "        self.log_ops = dict()\n",
        "        self.log_ops[\"global_step\"] = global_step\n",
        "        self.log_ops[\"likelihood_loss\"] = likelihood_loss\n",
        "        self.log_ops[\"kl_loss\"] = kl_loss\n",
        "        self.log_ops[\"kl_weight\"] = kl_weight\n",
        "        self.log_ops[\"loss\"] = loss\n",
        "        self.img_ops = dict()\n",
        "        self.img_ops[\"sample\"] = sample\n",
        "        self.img_ops[\"test_sample\"] = test_sample\n",
        "        self.img_ops[\"x\"] = self.x\n",
        "        self.img_ops[\"c\"] = self.c\n",
        "        for i, l in enumerate(self.vgg19.losses):\n",
        "            self.log_ops[\"vgg_loss_{}\".format(i)] = l\n",
        "\n",
        "        # keep seperate train and validation summaries\n",
        "        # only training summary contains histograms\n",
        "        train_summaries = list()\n",
        "        for k, v in self.log_ops.items():\n",
        "            train_summaries.append(tf.summary.scalar(k, v))\n",
        "        self.train_summary_op = tf.summary.merge_all()\n",
        "\n",
        "        valid_summaries = list()\n",
        "        for k, v in self.log_ops.items():\n",
        "            valid_summaries.append(tf.summary.scalar(k+\"_valid\", v))\n",
        "        self.valid_summary_op = tf.summary.merge(valid_summaries)\n",
        "\n",
        "        # all variables for initialization\n",
        "        self.variables = [v for v in tf.global_variables()\n",
        "                if not v in self.vgg19.variables]\n",
        "\n",
        "        self.logger.info(\"Defined graph\")\n",
        "\n",
        "\n",
        "    def init_graph(self, init_batch):\n",
        "        self.writer = tf.summary.FileWriter(\n",
        "                self.out_dir,\n",
        "                session.graph)\n",
        "        self.saver = tf.train.Saver(self.variables)\n",
        "        initializer_op = tf.variables_initializer(self.variables)\n",
        "        feed = {\n",
        "            self.xn_init: init_batch[2],\n",
        "            self.cn_init: init_batch[3],\n",
        "            self.x_init: init_batch[0],\n",
        "            self.c_init: init_batch[1]}\n",
        "        session.run(initializer_op, feed)\n",
        "        session.run(self.dd_init_op, feed)\n",
        "        self.logger.info(\"Initialized model from scratch\")\n",
        "\n",
        "\n",
        "    def restore_graph(self, restore_path):\n",
        "        self.writer = tf.summary.FileWriter(\n",
        "                self.out_dir,\n",
        "                session.graph)\n",
        "        self.saver = tf.train.Saver(self.variables)\n",
        "        self.saver.restore(session, restore_path)\n",
        "        self.logger.info(\"Restored model from {}\".format(restore_path))\n",
        "\n",
        "\n",
        "    def reset_global_step(self):\n",
        "        session.run(tf.assign(self.log_ops[\"global_step\"], 0))\n",
        "        self.logger.info(\"Reset global_step\")\n",
        "\n",
        "\n",
        "    def fit(self, batches, valid_batches = None):\n",
        "        start_step = self.log_ops[\"global_step\"].eval(session)\n",
        "        self.valid_batches = valid_batches\n",
        "        for batch in trange(start_step, self.lr_decay_end):\n",
        "            X_batch, C_batch, XN_batch, CN_batch = next(batches)\n",
        "            feed_dict = {\n",
        "                    self.xn: XN_batch,\n",
        "                    self.cn: CN_batch,\n",
        "                    self.x: X_batch,\n",
        "                    self.c: C_batch}\n",
        "            fetch_dict = {\"train\": self.train_op}\n",
        "            if self.log_ops[\"global_step\"].eval(session) % self.log_frequency == 0:\n",
        "                fetch_dict[\"log\"] = self.log_ops\n",
        "                fetch_dict[\"img\"] = self.img_ops\n",
        "                fetch_dict[\"summary\"] = self.train_summary_op\n",
        "            result = session.run(fetch_dict, feed_dict)\n",
        "            self.log_result(result)\n",
        "\n",
        "\n",
        "    def log_result(self, result, **kwargs):\n",
        "        global_step = self.log_ops[\"global_step\"].eval(session)\n",
        "        if \"summary\" in result:\n",
        "            self.writer.add_summary(result[\"summary\"], global_step)\n",
        "            self.writer.flush()\n",
        "        if \"log\" in result:\n",
        "            for k in sorted(result[\"log\"]):\n",
        "                v = result[\"log\"][k]\n",
        "                self.logger.info(\"{}: {}\".format(k, v))\n",
        "        if \"img\" in result:\n",
        "            for k, v in result[\"img\"].items():\n",
        "                plot_batch(v, os.path.join(\n",
        "                    self.out_dir,\n",
        "                    k + \"_{:07}.png\".format(global_step)))\n",
        "\n",
        "            if self.valid_batches is not None:\n",
        "                # validation run\n",
        "                X_batch, C_batch, XN_batch, CN_batch = next(self.valid_batches)\n",
        "                feed_dict = {\n",
        "                        self.xn: XN_batch,\n",
        "                        self.cn: CN_batch,\n",
        "                        self.x: X_batch,\n",
        "                        self.c: C_batch}\n",
        "                fetch_dict = dict()\n",
        "                fetch_dict[\"imgs\"] = self.img_ops\n",
        "                fetch_dict[\"summary\"] = self.valid_summary_op\n",
        "                fetch_dict[\"validation_loss\"] = self.log_ops[\"loss\"]\n",
        "                result = session.run(fetch_dict, feed_dict)\n",
        "                self.writer.add_summary(result[\"summary\"], global_step)\n",
        "                self.writer.flush()\n",
        "                # display samples\n",
        "                imgs = result[\"imgs\"]\n",
        "                for k, v in imgs.items():\n",
        "                    plot_batch(v, os.path.join(\n",
        "                        self.out_dir,\n",
        "                        \"valid_\" + k + \"_{:07}.png\".format(global_step)))\n",
        "                # log validation loss\n",
        "                validation_loss = result[\"validation_loss\"]\n",
        "                self.logger.info(\"{}: {}\".format(\"validation_loss\", validation_loss))\n",
        "                if self.checkpoint_best and validation_loss < self.best_loss:\n",
        "                    # checkpoint if validation loss improved\n",
        "                    self.logger.info(\"step {}: Validation loss improved from {:.4e} to {:.4e}\".format(global_step, self.best_loss, validation_loss))\n",
        "                    self.best_loss = validation_loss\n",
        "                    self.make_checkpoint(global_step, prefix = \"best_\")\n",
        "        if global_step % self.test_frequency == 0:\n",
        "            if self.valid_batches is not None:\n",
        "                # testing\n",
        "                X_batch, C_batch, XN_batch, CN_batch = next(self.valid_batches)\n",
        "                x_gen = self.test(C_batch)\n",
        "                for k in x_gen:\n",
        "                    plot_batch(x_gen[k], os.path.join(\n",
        "                        self.out_dir,\n",
        "                        \"testing_{}_{:07}.png\".format(k, global_step)))\n",
        "                # transfer\n",
        "                bs = X_batch.shape[0]\n",
        "                imgs = list()\n",
        "                imgs.append(np.zeros_like(X_batch[0,...]))\n",
        "                for r in range(bs):\n",
        "                    imgs.append(C_batch[r,...])\n",
        "                for i in range(bs):\n",
        "                    x_infer = XN_batch[i,...]\n",
        "                    c_infer = CN_batch[i,...]\n",
        "                    imgs.append(X_batch[i,...])\n",
        "\n",
        "                    x_infer_batch = x_infer[None,...].repeat(bs, axis = 0)\n",
        "                    c_infer_batch = c_infer[None,...].repeat(bs, axis = 0)\n",
        "                    c_generate_batch = C_batch\n",
        "                    results = model.transfer(x_infer_batch, c_infer_batch, c_generate_batch)\n",
        "                    for j in range(bs):\n",
        "                        imgs.append(results[j,...])\n",
        "                imgs = np.stack(imgs, axis = 0)\n",
        "                plot_batch(imgs, os.path.join(\n",
        "                    out_dir,\n",
        "                    \"transfer_{:07}.png\".format(global_step)))\n",
        "        if global_step % self.ckpt_frequency == 0:\n",
        "            self.make_checkpoint(global_step)\n",
        "\n",
        "\n",
        "    def make_checkpoint(self, global_step, prefix = \"\"):\n",
        "        fname = os.path.join(self.checkpoint_dir, prefix + \"model.ckpt\")\n",
        "        self.saver.save(\n",
        "                session,\n",
        "                fname,\n",
        "                global_step = global_step)\n",
        "        self.logger.info(\"Saved model to {}\".format(fname))\n",
        "\n",
        "\n",
        "    def test(self, c_batch):\n",
        "        results = dict()\n",
        "        results[\"cond\"] = c_batch\n",
        "        sample = session.run(self.img_ops[\"test_sample\"],\n",
        "            {self.c: c_batch})\n",
        "        results[\"test_sample\"] = sample\n",
        "        return results\n",
        "\n",
        "\n",
        "    def reconstruct(self, x_batch, c_batch):\n",
        "        return session.run(\n",
        "                self.reconstruction,\n",
        "                {self.x: x_batch, self.c: c_batch})\n",
        "\n",
        "\n",
        "    def transfer(self, x_encode, c_encode, c_decode):\n",
        "        initialized = getattr(self, \"_init_transfer\", False)\n",
        "        if not initialized:\n",
        "            # transfer\n",
        "            self.c_generator = tf.placeholder(\n",
        "                    tf.float32,\n",
        "                    shape = [self.batch_size] + self.img_shape)\n",
        "            infer_x = self.xn\n",
        "            infer_c = self.cn\n",
        "            generate_c = self.c_generator\n",
        "            transfer_params = self.transfer_pass(infer_x, infer_c, generate_c)\n",
        "            self.transfer_mean_sample = self.sample(transfer_params)\n",
        "            self._init_transfer = True\n",
        "\n",
        "        return session.run(\n",
        "                self.transfer_mean_sample, {\n",
        "                    self.xn: x_encode,\n",
        "                    self.cn: c_encode,\n",
        "                    self.c_generator: c_decode})\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    default_log_dir = os.path.join(os.getcwd(), \"log\")\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--config\", required = True, help = \"path to config\")\n",
        "    parser.add_argument(\"--mode\", default = \"train\",\n",
        "            choices=[\"train\", \"test\", \"add_reconstructions\", \"transfer\"])\n",
        "    parser.add_argument(\"--log_dir\", default = default_log_dir, help = \"path to log into\")\n",
        "    parser.add_argument(\"--checkpoint\", help = \"path to checkpoint to restore\")\n",
        "    parser.add_argument(\"--retrain\", dest = \"retrain\", action = \"store_true\", help = \"reset global_step to zero\")\n",
        "    parser.set_defaults(retrain = False)\n",
        "\n",
        "    opt = parser.parse_args()\n",
        "\n",
        "    with open(opt.config) as f:\n",
        "        config = yaml.load(f)\n",
        "\n",
        "    out_dir, logger = init_logging(opt.log_dir)\n",
        "    logger.info(opt)\n",
        "    logger.info(yaml.dump(config))\n",
        "\n",
        "    if opt.mode == \"train\":\n",
        "        batch_size = config[\"batch_size\"]\n",
        "        img_shape = 2*[config[\"spatial_size\"]] + [3]\n",
        "        data_shape = [batch_size] + img_shape\n",
        "        init_shape = [config[\"init_batches\"] * batch_size] + img_shape\n",
        "        box_factor = config[\"box_factor\"]\n",
        "\n",
        "        data_index = config[\"data_index\"]\n",
        "        batches = get_batches(data_shape, data_index, train = True, box_factor = box_factor)\n",
        "        init_batches = get_batches(init_shape, data_index, train = True, box_factor = box_factor)\n",
        "        valid_batches = get_batches(data_shape, data_index, train = False, box_factor = box_factor)\n",
        "        logger.info(\"Number of training samples: {}\".format(batches.n))\n",
        "        logger.info(\"Number of validation samples: {}\".format(valid_batches.n))\n",
        "\n",
        "        model = Model(config, out_dir, logger)\n",
        "        if opt.checkpoint is not None:\n",
        "            model.restore_graph(opt.checkpoint)\n",
        "        else:\n",
        "            model.init_graph(next(init_batches))\n",
        "        if opt.retrain:\n",
        "            model.reset_global_step()\n",
        "        model.fit(batches, valid_batches)\n",
        "    elif opt.mode == \"transfer\":\n",
        "        batch_size = config[\"batch_size\"]\n",
        "        img_shape = 2*[config[\"spatial_size\"]] + [3]\n",
        "        data_shape = [batch_size] + img_shape\n",
        "        box_factor = config[\"box_factor\"]\n",
        "        data_index = config[\"data_index\"]\n",
        "\n",
        "        valid_batches = get_batches(data_shape, data_index,\n",
        "                box_factor = box_factor, train = False)\n",
        "\n",
        "        model = Model(config, out_dir, logger)\n",
        "        assert opt.checkpoint is not None\n",
        "        model.restore_graph(opt.checkpoint)\n",
        "\n",
        "        for step in trange(10):\n",
        "            X_batch, C_batch, XN_batch, CN_batch = next(valid_batches)\n",
        "            bs = X_batch.shape[0]\n",
        "            imgs = list()\n",
        "            imgs.append(np.zeros_like(X_batch[0,...]))\n",
        "            for r in range(bs):\n",
        "                imgs.append(C_batch[r,...])\n",
        "            for i in range(bs):\n",
        "                x_infer = XN_batch[i,...]\n",
        "                c_infer = CN_batch[i,...]\n",
        "                imgs.append(X_batch[i,...])\n",
        "\n",
        "                x_infer_batch = x_infer[None,...].repeat(bs, axis = 0)\n",
        "                c_infer_batch = c_infer[None,...].repeat(bs, axis = 0)\n",
        "                c_generate_batch = C_batch\n",
        "                results = model.transfer(x_infer_batch, c_infer_batch, c_generate_batch)\n",
        "                for j in range(bs):\n",
        "                    imgs.append(results[j,...])\n",
        "            imgs = np.stack(imgs, axis = 0)\n",
        "            plot_batch(imgs, os.path.join(\n",
        "                out_dir,\n",
        "                \"transfer_{}.png\".format(step)))\n",
        "    else:\n",
        "        raise NotImplemented()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}